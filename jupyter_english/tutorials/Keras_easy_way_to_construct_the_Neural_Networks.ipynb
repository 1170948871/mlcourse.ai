{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "![alt text](http://i64.tinypic.com/as8k4.jpg “Title”)  \n",
    "## [mlcourse.ai](https://mlcourse.ai/)   Open Machine learning course\n",
    "</center>\n",
    "<center>Author:Natalia Domozhirova, slack: @ndomozhirova."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Tutorial</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "![alt text](http://i63.tinypic.com/35mpimt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>KERAS : easy way to construct the Neural Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a high-level neural networks API, written in Python.\n",
    "\n",
    "Major Keras features:\n",
    "- its  capable of running on top of TensorFlow, CNTK, or Theano.\n",
    "- Keras allows for easy and fast prototyping and supports both Perceptrons, Convolutional networks and Recurrent networks (including LSTM), as well as their combinations. \n",
    "- Keras is compatible with: Python 2.7-3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the process more interesting let's consider the classification example  from the real life.\n",
    "\n",
    "## <center>Example description. \n",
    "\n",
    "Let's take the task from one Hakaton, organized by some polypropylene producer this year. So, let’s consider the production of the polypropylene granules by extruder. Extruder is a kind of “meat grinder” which has the knives at the end of the process which are cutting the output product onto granules of the whole process.  \n",
    "The problem is that sometimes the production mass has an irregular consistency and sticks to the knives. When there is a lot of stuck mass - knives can no longer function. So,  it is necessary to stop production process, which is very expensive. If we catch the very beginning of such sticking process -  there is a  method to very quickly and painless  clean the knives and continue production without stopping.\n",
    "So, the task is to send the stop signal to operator a bit in advance (let’s say not later then 15 minutes) – so that he could have a time for necessary manipulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src= \"http://i68.tinypic.com/2rr2glg.jpg\n",
    "\" style=\"height:250px\">      \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re considering already preprocessed normalized dataset with the vectors of the system sensor’s values  (5,160 features) and “0” or “1” targets. It is already devided by [train](https://drive.google.com/open?id=1TMlClLguxcXTOAJt8VKe-iLrndJuFShl) and [test](https://drive.google.com/open?id=1JonMu0wmMbUqcbSd17Qr2A3AhVF3nutZ). \n",
    "Let's download and prepare to work our datasets. In the datasets there are targets in zero column and the timestamps -in the 1st column.So, let's extract our train and test matrix as well as our targets. Also we'll transform targets to categorical -so to have as a result y 2-dimentional vector, i.e. the vector with probabilities of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9498, 5160)\n",
      "(9498, 2)\n",
      "(2574, 5160)\n",
      "(2574, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "df_train = pd.read_csv('train2.tsv', sep='\\t', header=None)\n",
    "df_test = pd.read_csv('test2.tsv', sep='\\t', header=None)\n",
    "\n",
    "Y_train = np.array(df_train[0].values.astype(int))\n",
    "Y_test = np.array(df_test[0].values.astype(int))\n",
    "X_train = np.array(df_train.iloc[:,2:].values.astype(float))\n",
    "X_test = np.array(df_test.iloc[:,2:].values.astype(float))\n",
    "\n",
    "Y_train = Y_train.astype(np.int)\n",
    "Y_test = Y_test.astype(np.int)\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train, 2)\n",
    "Y_test = np_utils.to_categorical(Y_test, 2)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "print (X_test.shape)\n",
    "print (Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider how the simple Newral Network(NN), Multilayer Perceptron (MLP), with 3 hidden layers, constructed by Keras, can help us to solve this problem.\n",
    "\n",
    "As we have hidden layers - this would be a Deep Neural Network. \n",
    "Also, we can see, that we need to have 5160 neurons in the input layer, as this is the size of our vector X and 2 neurons in the last layer - as this is the size of our target.\n",
    "You can read, for example, [here](https://en.wikipedia.org/wiki/Multilayer_perceptron) some more information about MLP structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src= \"http://i66.tinypic.com/2d6tsm.jpg\n",
    "\" style=\"height:250px\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data structure of Keras is a  **_model_** - a way to organize layers. The simplest type of model is the **_Sequential_** model, a linear stack of layers, which is appropriate for MLP construction (for more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "model1 = Sequential() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the type of model is defined - we need to consistently add layers -**_Dense_**. Stacking layers is as easy as **_.add()_**.\n",
    "\n",
    "While adding the layer we need to define the ***nput_dim*** number of neurons and activation functionsa. For the fist layer we also need to add the dimention of X vectors. In our case this is 5,160. The last layer consists on 2 neurons exactly as our target vestors Y_train and Y_test do.\n",
    "\n",
    "We can choose the **_Activation_** functions as well as the _number of neurons_ for each certain layer which look appropriate for the base line and later tune this if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense\n",
    "\n",
    "model1.add(Dense(64, input_dim=5160))\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "model1.add(Dense(64))\n",
    "model1.add(Activation('sigmoid'))\n",
    "\n",
    "model1.add(Dense(128))\n",
    "model1.add(Activation('tanh'))\n",
    "\n",
    "model1.add(Dense(2))\n",
    "model1.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our model looks good, we need configure its learning process with **_.compile()_**.\n",
    "\n",
    "Here we should also describe the **_loss_** function and **_metrics_** we want to use as well as **_optimizer_** (the type of the Gradient descent to be used) which seem appropriate in each particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate on our training data in batches with the ***batch_size*** we want, where X_train and y_train are Numpy arrays just like in the Scikit-Learn API.\n",
    "Also we can define the number of ***epochs*** (i.e. the max number of the full cycles of model's training). \n",
    "***Verbose=1*** just let us see the summary of the current stage of calcualtions.\n",
    "\n",
    "We can also printing our model parameters using ***model.summary()***. \n",
    "It is also can be useful to see the shapes of X_train, y_train,X_test,y_test\n",
    "\n",
    "Also, if we would be satisfied by the results of the model we can save the best one via ***callback_save*** option.\n",
    " \n",
    "And there is an ***callback_early stop*** option to stop the training process when we don't have significant improvement during the certain number of epochs (***patience***). \n",
    " \n",
    "Now our first model is ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                330304    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 343,042\n",
      "Trainable params: 343,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(9498, 5160)\n",
      "(9498, 2)\n",
      "(2574, 5160)\n",
      "(2574, 2)\n",
      "Train on 9498 samples, validate on 2574 samples\n",
      "Epoch 1/10000\n",
      "9498/9498 [==============================] - 6s 614us/step - loss: 0.3004 - acc: 0.8734 - val_loss: 0.3620 - val_acc: 0.8116\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.81158, saving model to best_model1.model1\n",
      "Epoch 2/10000\n",
      "9498/9498 [==============================] - 4s 402us/step - loss: 0.2464 - acc: 0.9012 - val_loss: 0.4088 - val_acc: 0.8159\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.81158 to 0.81585, saving model to best_model1.model1\n",
      "Epoch 3/10000\n",
      "9498/9498 [==============================] - 4s 389us/step - loss: 0.2237 - acc: 0.9134 - val_loss: 0.5629 - val_acc: 0.7090\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/10000\n",
      "9498/9498 [==============================] - 4s 389us/step - loss: 0.1804 - acc: 0.9342 - val_loss: 0.7109 - val_acc: 0.7308\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/10000\n",
      "9498/9498 [==============================] - 4s 393us/step - loss: 0.1491 - acc: 0.9485 - val_loss: 1.1560 - val_acc: 0.6807\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/10000\n",
      "9498/9498 [==============================] - 4s 398us/step - loss: 0.1307 - acc: 0.9569 - val_loss: 1.2498 - val_acc: 0.6845\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/10000\n",
      "9498/9498 [==============================] - 4s 394us/step - loss: 0.0988 - acc: 0.9653 - val_loss: 1.5320 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/10000\n",
      "9498/9498 [==============================] - 4s 395us/step - loss: 0.0921 - acc: 0.9709 - val_loss: 1.0608 - val_acc: 0.7075\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10000\n",
      "9498/9498 [==============================] - 4s 393us/step - loss: 0.0835 - acc: 0.9733 - val_loss: 2.0272 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10000\n",
      "9498/9498 [==============================] - 4s 402us/step - loss: 0.0750 - acc: 0.9764 - val_loss: 2.0684 - val_acc: 0.5723\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/10000\n",
      "9498/9498 [==============================] - 4s 398us/step - loss: 0.0703 - acc: 0.9786 - val_loss: 2.4017 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff19025dfd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "print (X_test.shape)\n",
    "print (Y_test.shape)\n",
    "\n",
    "callback_save       = ModelCheckpoint('best_model1.model1', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "callback_earlystop  = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "model1.fit(  \n",
    "    X_train,   \n",
    "    Y_train,   \n",
    "    batch_size=20,   \n",
    "    epochs=10000,  \n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test), \n",
    "    callbacks=[callback_save, callback_earlystop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, we got a Baseline with Accuracy= 0.81585**. This happened on the 2nd epoch already.It looks cool, as we even didn't tune anythong yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve this result. For example, we can introduce **Dropout** - this is a kind of regularization for the Neral Networks.\n",
    "The level of drop out is a probability of the exclusion the random choice neuron from the calculations. So, drop outs help to prevent the NN overfitting.  \n",
    "Let's create the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 64)                330304    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 343,042\n",
      "Trainable params: 343,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(9498, 5160)\n",
      "(9498, 2)\n",
      "(2574, 5160)\n",
      "(2574, 2)\n",
      "Train on 9498 samples, validate on 2574 samples\n",
      "Epoch 1/10000\n",
      "9498/9498 [==============================] - 5s 501us/step - loss: 0.4304 - acc: 0.8068 - val_loss: 0.3512 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84965, saving model to best_model2.model2\n",
      "Epoch 2/10000\n",
      "9498/9498 [==============================] - 5s 490us/step - loss: 0.3354 - acc: 0.8566 - val_loss: 0.3164 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/10000\n",
      "9498/9498 [==============================] - 4s 422us/step - loss: 0.3128 - acc: 0.8700 - val_loss: 0.3272 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/10000\n",
      "9498/9498 [==============================] - 4s 411us/step - loss: 0.3017 - acc: 0.8779 - val_loss: 0.3309 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.84965 to 0.85082, saving model to best_model2.model2\n",
      "Epoch 5/10000\n",
      "9498/9498 [==============================] - 4s 419us/step - loss: 0.2868 - acc: 0.8858 - val_loss: 0.3606 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/10000\n",
      "9498/9498 [==============================] - 4s 419us/step - loss: 0.2697 - acc: 0.8944 - val_loss: 0.3521 - val_acc: 0.8621\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85082 to 0.86208, saving model to best_model2.model2\n",
      "Epoch 7/10000\n",
      "9498/9498 [==============================] - 4s 429us/step - loss: 0.2674 - acc: 0.8944 - val_loss: 0.3097 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/10000\n",
      "9498/9498 [==============================] - 4s 420us/step - loss: 0.2645 - acc: 0.8970 - val_loss: 0.3198 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10000\n",
      "9498/9498 [==============================] - 4s 421us/step - loss: 0.2338 - acc: 0.9132 - val_loss: 0.5480 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10000\n",
      "9498/9498 [==============================] - 4s 424us/step - loss: 0.2304 - acc: 0.9136 - val_loss: 0.5251 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/10000\n",
      "9498/9498 [==============================] - 4s 433us/step - loss: 0.2200 - acc: 0.9208 - val_loss: 0.7973 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/10000\n",
      "9498/9498 [==============================] - 4s 425us/step - loss: 0.2373 - acc: 0.9150 - val_loss: 0.2991 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.86208 to 0.86791, saving model to best_model2.model2\n",
      "Epoch 13/10000\n",
      "9498/9498 [==============================] - 4s 420us/step - loss: 0.2139 - acc: 0.9221 - val_loss: 0.2716 - val_acc: 0.8757\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.86791 to 0.87568, saving model to best_model2.model2\n",
      "Epoch 14/10000\n",
      "9498/9498 [==============================] - 4s 416us/step - loss: 0.2171 - acc: 0.9152 - val_loss: 0.9151 - val_acc: 0.7852\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/10000\n",
      "9498/9498 [==============================] - 4s 430us/step - loss: 0.1959 - acc: 0.9300 - val_loss: 0.8608 - val_acc: 0.7879\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/10000\n",
      "9498/9498 [==============================] - 4s 391us/step - loss: 0.1948 - acc: 0.9294 - val_loss: 0.9067 - val_acc: 0.7642\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/10000\n",
      "9498/9498 [==============================] - 4s 393us/step - loss: 0.2191 - acc: 0.9154 - val_loss: 1.0428 - val_acc: 0.7078\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/10000\n",
      "9498/9498 [==============================] - 4s 390us/step - loss: 0.1937 - acc: 0.9300 - val_loss: 0.5398 - val_acc: 0.8159\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/10000\n",
      "9498/9498 [==============================] - 4s 402us/step - loss: 0.1776 - acc: 0.9366 - val_loss: 1.1669 - val_acc: 0.7168\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/10000\n",
      "9498/9498 [==============================] - 4s 391us/step - loss: 0.1775 - acc: 0.9375 - val_loss: 1.1106 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 21/10000\n",
      "9498/9498 [==============================] - 4s 394us/step - loss: 0.1721 - acc: 0.9396 - val_loss: 1.1867 - val_acc: 0.7183\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/10000\n",
      "9498/9498 [==============================] - 4s 393us/step - loss: 0.1717 - acc: 0.9381 - val_loss: 1.0403 - val_acc: 0.7653\n",
      "\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 23/10000\n",
      "9498/9498 [==============================] - 4s 393us/step - loss: 0.1730 - acc: 0.9416 - val_loss: 1.1055 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff1902d53c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model2 = Sequential() \n",
    "\n",
    "model2.add(Dense(64, input_dim=5160))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.3, seed=123))\n",
    "\n",
    "model2.add(Dense(64))\n",
    "model2.add(Activation('sigmoid'))\n",
    "model2.add(Dropout(0.4, seed=123))\n",
    "\n",
    "model2.add(Dense(128))\n",
    "model2.add(Activation('tanh'))\n",
    "model2.add(Dropout(0.5, seed=123))\n",
    "\n",
    "model2.add(Dense(2))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "print (X_test.shape)\n",
    "print (Y_test.shape)\n",
    "\n",
    "callback_save       = ModelCheckpoint('best_model2.model2', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "callback_earlystop  = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "model2.fit(  \n",
    "    X_train,   \n",
    "    Y_train,   \n",
    "    batch_size=20,   \n",
    "    epochs=10000,  \n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test), \n",
    "    callbacks=[callback_save, callback_earlystop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thud, adding the drop-outs we've **increased Accuracy on the test up to 0.85082**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tune the rest of gyper-parameters like the *number of layers*, the *levels of drop-outs*, *activation functions*, *optimizer* etc.\n",
    "For this purposes we can use, for example, another very friendly and easy-to-apply  - Hyperas library. The description with examples you can find [here](https://github.com/maxpumperla/hyperas).\n",
    "As a result of such tuning we've got the following model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 64)                330304    \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 873,026\n",
      "Trainable params: 873,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(9498, 5160)\n",
      "(9498, 2)\n",
      "(2574, 5160)\n",
      "(2574, 2)\n",
      "Train on 9498 samples, validate on 2574 samples\n",
      "Epoch 1/10000\n",
      "9498/9498 [==============================] - 4s 378us/step - loss: 0.6785 - acc: 0.8032 - val_loss: 0.3593 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85004, saving model to best_model3.model3\n",
      "Epoch 2/10000\n",
      "9498/9498 [==============================] - 2s 259us/step - loss: 0.3725 - acc: 0.8522 - val_loss: 0.3376 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85004 to 0.86674, saving model to best_model3.model3\n",
      "Epoch 3/10000\n",
      "9498/9498 [==============================] - 3s 267us/step - loss: 0.3599 - acc: 0.8612 - val_loss: 0.3393 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/10000\n",
      "9498/9498 [==============================] - 3s 278us/step - loss: 0.3514 - acc: 0.8688 - val_loss: 0.3411 - val_acc: 0.8582\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/10000\n",
      "9498/9498 [==============================] - 3s 304us/step - loss: 0.3153 - acc: 0.8762 - val_loss: 0.3764 - val_acc: 0.8559\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/10000\n",
      "9498/9498 [==============================] - 2s 254us/step - loss: 0.3195 - acc: 0.8783 - val_loss: 0.3755 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/10000\n",
      "9498/9498 [==============================] - 2s 245us/step - loss: 0.3004 - acc: 0.8939 - val_loss: 0.4397 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/10000\n",
      "9498/9498 [==============================] - 2s 247us/step - loss: 0.2858 - acc: 0.9006 - val_loss: 0.4427 - val_acc: 0.8524\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10000\n",
      "9498/9498 [==============================] - 3s 314us/step - loss: 0.2771 - acc: 0.9052 - val_loss: 0.4645 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10000\n",
      "9498/9498 [==============================] - 3s 279us/step - loss: 0.2694 - acc: 0.9064 - val_loss: 0.4165 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/10000\n",
      "9498/9498 [==============================] - 3s 276us/step - loss: 0.2695 - acc: 0.9111 - val_loss: 0.4255 - val_acc: 0.8605\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/10000\n",
      "9498/9498 [==============================] - 3s 283us/step - loss: 0.2704 - acc: 0.9069 - val_loss: 0.4052 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0bf3ea240>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(64, input_dim=5160))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.11729755246044238, seed=123))\n",
    "    \n",
    "model3.add(Dense(256))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.8444244099007299,seed=123))\n",
    "\n",
    "model3.add(Dense(1024))\n",
    "model3.add(Activation('linear'))\n",
    "model3.add(Dropout(0.41266207281071243,seed=123))\n",
    "\n",
    "model3.add(Dense(256))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.4844455237320119,seed=123))\n",
    "\n",
    "model3.add(Dense(2))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model3.summary()\n",
    "\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "print (X_test.shape)\n",
    "print (Y_test.shape)\n",
    "\n",
    "callback_save       = ModelCheckpoint('best_model3.model3', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "callback_earlystop  = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model3.fit(  \n",
    "    X_train,   \n",
    "    Y_train,   \n",
    "    batch_size=60,   \n",
    "    epochs=10000,  \n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test), \n",
    "    callbacks=[callback_save, callback_earlystop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Now, with tunned parameters we've managed to imporove Accuracy up to 0.86674"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Keras it is also possible to use  **L1/L2 weight regularization** which allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_87 (Dense)             (None, 64)                330304    \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 873,026\n",
      "Trainable params: 873,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(9498, 5160)\n",
      "(9498, 2)\n",
      "(2574, 5160)\n",
      "(2574, 2)\n",
      "Train on 9498 samples, validate on 2574 samples\n",
      "Epoch 1/10000\n",
      "9498/9498 [==============================] - 4s 411us/step - loss: 12.9089 - acc: 0.6219 - val_loss: 6.1716 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.83916, saving model to best_model4.model4\n",
      "Epoch 2/10000\n",
      "9498/9498 [==============================] - 3s 274us/step - loss: 11.5921 - acc: 0.6976 - val_loss: 6.7948 - val_acc: 0.7902\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/10000\n",
      "9498/9498 [==============================] - 2s 262us/step - loss: 11.4244 - acc: 0.7130 - val_loss: 6.9765 - val_acc: 0.7218\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/10000\n",
      "9498/9498 [==============================] - 3s 265us/step - loss: 11.6085 - acc: 0.7139 - val_loss: 7.3498 - val_acc: 0.7953\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/10000\n",
      "9498/9498 [==============================] - 3s 283us/step - loss: 11.5146 - acc: 0.7078 - val_loss: 6.9914 - val_acc: 0.6682\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/10000\n",
      "9498/9498 [==============================] - 3s 277us/step - loss: 11.5989 - acc: 0.7038 - val_loss: 5.8045 - val_acc: 0.8236\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/10000\n",
      "9498/9498 [==============================] - 3s 264us/step - loss: 11.4583 - acc: 0.7216 - val_loss: 5.7994 - val_acc: 0.5653\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/10000\n",
      "9498/9498 [==============================] - 3s 275us/step - loss: 11.5173 - acc: 0.6845 - val_loss: 9.4636 - val_acc: 0.5214\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10000\n",
      "9498/9498 [==============================] - 3s 270us/step - loss: 11.6029 - acc: 0.6953 - val_loss: 10.0458 - val_acc: 0.7855\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10000\n",
      "9498/9498 [==============================] - 3s 272us/step - loss: 11.5828 - acc: 0.6921 - val_loss: 6.5450 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/10000\n",
      "9498/9498 [==============================] - 3s 270us/step - loss: 11.4618 - acc: 0.7462 - val_loss: 13.8103 - val_acc: 0.5451\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/10000\n",
      "9498/9498 [==============================] - 3s 299us/step - loss: 11.4451 - acc: 0.7207 - val_loss: 7.1957 - val_acc: 0.6445\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 13/10000\n",
      "9498/9498 [==============================] - 3s 279us/step - loss: 11.4992 - acc: 0.7094 - val_loss: 6.4634 - val_acc: 0.5128\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/10000\n",
      "9498/9498 [==============================] - 3s 283us/step - loss: 11.4754 - acc: 0.6972 - val_loss: 5.3869 - val_acc: 0.7859\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/10000\n",
      "9498/9498 [==============================] - 3s 302us/step - loss: 11.4403 - acc: 0.7104 - val_loss: 25.0500 - val_acc: 0.5423\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/10000\n",
      "9498/9498 [==============================] - 3s 320us/step - loss: 11.6409 - acc: 0.7210 - val_loss: 5.7811 - val_acc: 0.7681\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/10000\n",
      "9498/9498 [==============================] - 3s 305us/step - loss: 11.5595 - acc: 0.7257 - val_loss: 8.3789 - val_acc: 0.8411\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.83916 to 0.84110, saving model to best_model4.model4\n",
      "Epoch 18/10000\n",
      "9498/9498 [==============================] - 3s 289us/step - loss: 11.5333 - acc: 0.7134 - val_loss: 5.9019 - val_acc: 0.8026\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/10000\n",
      "9498/9498 [==============================] - 3s 300us/step - loss: 11.5437 - acc: 0.7236 - val_loss: 5.2646 - val_acc: 0.8061\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/10000\n",
      "9498/9498 [==============================] - 3s 279us/step - loss: 11.5058 - acc: 0.7280 - val_loss: 10.0391 - val_acc: 0.5591\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 21/10000\n",
      "9498/9498 [==============================] - 3s 272us/step - loss: 11.6585 - acc: 0.7033 - val_loss: 6.6956 - val_acc: 0.7653\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/10000\n",
      "9498/9498 [==============================] - 3s 279us/step - loss: 11.3160 - acc: 0.7043 - val_loss: 6.4920 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 23/10000\n",
      "9498/9498 [==============================] - 3s 290us/step - loss: 11.4835 - acc: 0.7099 - val_loss: 7.2062 - val_acc: 0.7720\n",
      "\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 24/10000\n",
      "9498/9498 [==============================] - 3s 289us/step - loss: 11.5844 - acc: 0.7331 - val_loss: 7.6895 - val_acc: 0.7782\n",
      "\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 25/10000\n",
      "9498/9498 [==============================] - 3s 285us/step - loss: 11.5484 - acc: 0.7155 - val_loss: 5.9454 - val_acc: 0.7199\n",
      "\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 26/10000\n",
      "9498/9498 [==============================] - 3s 290us/step - loss: 11.4440 - acc: 0.7271 - val_loss: 7.4951 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 27/10000\n",
      "9498/9498 [==============================] - 3s 288us/step - loss: 11.5024 - acc: 0.7239 - val_loss: 8.9691 - val_acc: 0.6601\n",
      "\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 28/10000\n",
      "9498/9498 [==============================] - 3s 280us/step - loss: 11.5683 - acc: 0.7258 - val_loss: 9.9270 - val_acc: 0.5167\n",
      "\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 29/10000\n",
      "9498/9498 [==============================] - 3s 280us/step - loss: 11.4381 - acc: 0.7057 - val_loss: 6.5970 - val_acc: 0.8023\n",
      "\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 00029: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0bc69a208>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import regularizers \n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(64, input_dim=5160,kernel_regularizer=regularizers.l2(0.0015), \n",
    "                 activity_regularizer=regularizers.l1(0.0015)))\n",
    "model3.add(Activation('relu'))\n",
    "model4.add(Dropout(0.11729755246044238, seed=123))\n",
    "    \n",
    "model4.add(Dense(256))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Dropout(0.8444244099007299,seed=123))\n",
    "\n",
    "model4.add(Dense(1024))\n",
    "model4.add(Activation('linear'))\n",
    "model4.add(Dropout(0.41266207281071243,seed=123))\n",
    "\n",
    "model4.add(Dense(256))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Dropout(0.4844455237320119,seed=123))\n",
    "\n",
    "model4.add(Dense(2))\n",
    "model4.add(Activation('softmax'))\n",
    "\n",
    "model4.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model4.summary()\n",
    "\n",
    "print (X_train.shape)\n",
    "print (Y_train.shape)\n",
    "print (X_test.shape)\n",
    "print (Y_test.shape)\n",
    "\n",
    "callback_save       = ModelCheckpoint('best_model4.model4', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "callback_earlystop  = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model4.fit(  \n",
    "    X_train,   \n",
    "    Y_train,   \n",
    "    batch_size=60,   \n",
    "    epochs=10000,  \n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test), \n",
    "    callbacks=[callback_save, callback_earlystop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see, that adding regualrization with the current coeffitients to the firs layer we've got just **Accuracy of 0.8392** which didn't improve the situation which means, that, as usual, they should be carefully tuned :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we **want to use the best trained model** we got, we can just download it and apply on the data needed.\n",
    "Let's see what we'll get on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9921489e-01, 7.8508107e-04],\n",
       "       [9.9908841e-01, 9.1164949e-04],\n",
       "       [1.9480076e-01, 8.0519927e-01],\n",
       "       [9.9921489e-01, 7.8508107e-04],\n",
       "       [8.9435852e-01, 1.0564152e-01]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model=load_model( 'best_model3.model3')\n",
    "result = model2.predict_on_batch(X_test)\n",
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also be interested to get a **list of all weight tensors** of the model, as Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.03492968, -0.0405129 , -0.00745696, ..., -0.03112122,\n",
       "         -0.02895394, -0.03320112],\n",
       "        [-0.00709045,  0.02336991, -0.00662623, ..., -0.02707123,\n",
       "          0.01238516,  0.00902536],\n",
       "        [ 0.00911781, -0.02620855, -0.01481253, ..., -0.00985456,\n",
       "         -0.02947772, -0.02592223],\n",
       "        ...,\n",
       "        [ 0.022276  , -0.02593126, -0.02306574, ..., -0.03842297,\n",
       "          0.00451999, -0.02098652],\n",
       "        [-0.03659435, -0.02750918, -0.00098653, ..., -0.04156897,\n",
       "          0.02848635, -0.00270199],\n",
       "        [ 0.03085619,  0.00692714,  0.00164401, ...,  0.02794782,\n",
       "         -0.02217145,  0.03590545]], dtype=float32)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights =model.get_weights()\n",
    "weights[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, you would propbably like to get the **model config** to re-use it on the future. This can be done as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_name': 'Dense',\n",
       "  'config': {'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'batch_input_shape': (None, 5160),\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'dtype': 'float32',\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'name': 'dense_9',\n",
       "   'trainable': True,\n",
       "   'units': 64,\n",
       "   'use_bias': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'relu', 'name': 'activation_9', 'trainable': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': 'dropout_4',\n",
       "   'noise_shape': None,\n",
       "   'rate': 0.11729755246044238,\n",
       "   'seed': 123,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'name': 'dense_10',\n",
       "   'trainable': True,\n",
       "   'units': 256,\n",
       "   'use_bias': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'relu',\n",
       "   'name': 'activation_10',\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': 'dropout_5',\n",
       "   'noise_shape': None,\n",
       "   'rate': 0.8444244099007299,\n",
       "   'seed': 123,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'name': 'dense_11',\n",
       "   'trainable': True,\n",
       "   'units': 1024,\n",
       "   'use_bias': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'linear',\n",
       "   'name': 'activation_11',\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': 'dropout_6',\n",
       "   'noise_shape': None,\n",
       "   'rate': 0.41266207281071243,\n",
       "   'seed': 123,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'name': 'dense_12',\n",
       "   'trainable': True,\n",
       "   'units': 256,\n",
       "   'use_bias': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'relu',\n",
       "   'name': 'activation_12',\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': 'dropout_7',\n",
       "   'noise_shape': None,\n",
       "   'rate': 0.5,\n",
       "   'seed': 123,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dropout',\n",
       "  'config': {'name': 'dropout_8',\n",
       "   'noise_shape': None,\n",
       "   'rate': 0.4844455237320119,\n",
       "   'seed': 123,\n",
       "   'trainable': True}},\n",
       " {'class_name': 'Dense',\n",
       "  'config': {'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'bias_constraint': None,\n",
       "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "   'bias_regularizer': None,\n",
       "   'kernel_constraint': None,\n",
       "   'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "    'config': {'distribution': 'uniform',\n",
       "     'mode': 'fan_avg',\n",
       "     'scale': 1.0,\n",
       "     'seed': None}},\n",
       "   'kernel_regularizer': None,\n",
       "   'name': 'dense_13',\n",
       "   'trainable': True,\n",
       "   'units': 2,\n",
       "   'use_bias': True}},\n",
       " {'class_name': 'Activation',\n",
       "  'config': {'activation': 'softmax',\n",
       "   'name': 'activation_13',\n",
       "   'trainable': True}}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config= model.get_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the model can be *reinstantiated* from its config via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more model tuning opportunities proposed by Keras pls see [here](https://keras.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the other type of the Neural Networks?\n",
    "Yes, you can use the similar approach types to the layers' construction principles for LSTM, CNN and some other types of the Deep Neural Networks. For more details pls see [here](https://keras.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sources used: https://keras.io/, https://towardsdatascience.com/meet-artificial-neural-networks-ae5939b1dd3a, https://www.quantinsti.com/blog/installing-keras-python-r, https://livebook.manning.com/#!/book/deep-learning-with-python/chapter-7, https://github.com/hyperopt/hyperopt"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
